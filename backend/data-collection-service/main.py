from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import os
import time
from datetime import datetime
from typing import Optional, List

from models import CrawlRequest, CrawlResponse, CrawlStatus
from naver_api import naver_api
from database import db_manager

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="News Data Collection Service",
    description="ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
crawl_status = CrawlStatus(
    is_running=False,
    last_run=None,
    total_collected=0,
    last_query=None,
    last_error=None
)

@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ì‹œ DynamoDB ì—°ê²°"""
    try:
        db_manager.connect()
        stats = db_manager.get_crawl_statistics()
        crawl_status.total_collected = stats['total_items']
        print(f"ğŸ“ˆ ê¸°ì¡´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {stats['total_items']}ê°œ")
    except Exception as e:
        print(f"âŒ ì‹œì‘ ì‹œ ì˜¤ë¥˜: {e}")

@app.get("/")
async def root():
    return {
        "service": "News Data Collection Service",
        "version": "1.0.0",
        "description": "ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘",
        "endpoints": [
            "POST /api/crawl/start - ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘",
            "POST /api/crawl/batch - ë°°ì¹˜ ìˆ˜ì§‘",
            "GET /api/crawl/status - ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ",
            "GET /health - í—¬ìŠ¤ì²´í¬"
        ]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "data-collection-service",
        "naver_api": "connected" if naver_api.client_id else "not_configured",
        "dynamodb": "connected" if db_manager.table else "not_connected"
    }

def crawl_news_sync(query: str, display: int = 10, start: int = 1, sort: str = "date") -> dict:
    """ë™ê¸° ë°©ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš©)"""
    crawl_status.is_running = True
    crawl_status.last_query = query
    crawl_status.last_error = None
    
    try:
        start_time = time.time()
        print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: '{query}' (display={display})")
        
        # CPU ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜ (Auto Scaling í…ŒìŠ¤íŠ¸ìš©)
        cpu_load_start = time.time()
        while time.time() - cpu_load_start < 1:  # 1ì´ˆê°„ CPU ë¶€í•˜
            _ = sum(range(30000))
        
        # ë„¤ì´ë²„ API í˜¸ì¶œ
        news_data = naver_api.search_news(query=query, display=display, start=start, sort=sort)
        
        # DynamoDB í˜•íƒœë¡œ ë³€í™˜
        db_items = naver_api.format_for_dynamodb(news_data, query)
        
        # DynamoDBì— ì €ì¥
        save_result = db_manager.save_news_items(db_items)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        crawl_status.total_collected += save_result['saved_count']
        crawl_status.last_run = datetime.now().isoformat()
        
        duration = time.time() - start_time
        
        result = {
            'message': f'Successfully saved {save_result["saved_count"]} news items to DynamoDB',
            'search_query': query,
            'collected_at': datetime.now().isoformat(),
            'total_results': news_data.get('total', 0),
            'saved_items': save_result['saved_items'],
            'failed_count': save_result['failed_count'],
            'duration_seconds': round(duration, 2)
        }
        
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {save_result['saved_count']}ê°œ ì €ì¥, {duration:.2f}ì´ˆ ì†Œìš”")
        return result
        
    except Exception as e:
        error_msg = f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        crawl_status.last_error = error_msg
        print(f"âŒ {error_msg}")
        raise Exception(error_msg)
    finally:
        crawl_status.is_running = False

@app.post("/api/crawl/start", response_model=CrawlResponse)
async def start_crawling(
    background_tasks: BackgroundTasks,
    query: str = Query("ë¹„íŠ¸ì½”ì¸", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    display: int = Query(10, ge=1, le=100, description="ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    start: int = Query(1, ge=1, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜"),
    sort: str = Query("date", description="ì •ë ¬ ë°©ì‹ (sim: ì •í™•ë„ìˆœ, date: ë‚ ì§œìˆœ)")
):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    def background_crawl():
        try:
            crawl_news_sync(query, display, start, sort)
        except Exception as e:
            print(f"ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    background_tasks.add_task(background_crawl)
    
    return CrawlResponse(
        statusCode=200,
        body={
            "message": f"'{query}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "query": query,
            "display": display,
            "sort": sort,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.post("/api/crawl/now", response_model=CrawlResponse)
async def crawl_now(
    query: str = Query("ë¹„íŠ¸ì½”ì¸", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    display: int = Query(10, ge=1, le=100, description="ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    start: int = Query(1, ge=1, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜"),
    sort: str = Query("date", description="ì •ë ¬ ë°©ì‹")
):
    """ì¦‰ì‹œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë™ê¸° ì‹¤í–‰)"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    try:
        result = crawl_news_sync(query, display, start, sort)
        
        return CrawlResponse(
            statusCode=200,
            body=result
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/crawl/batch")
async def batch_crawling(
    queries: List[str] = Query(["ë¹„íŠ¸ì½”ì¸", "AI", "í´ë¼ìš°ë“œ"], description="ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡"),
    display_per_query: int = Query(5, ge=1, le=50, description="í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜")
):
    """ë°°ì¹˜ ìˆ˜ì§‘ (ì—¬ëŸ¬ í‚¤ì›Œë“œ, CPU ì§‘ì•½ì )"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    start_time = time.time()
    total_saved = 0
    results = []
    
    try:
        for i, query in enumerate(queries):
            print(f"ğŸ” ë°°ì¹˜ ìˆ˜ì§‘ [{i+1}/{len(queries)}]: '{query}'")
            
            result = crawl_news_sync(query, display_per_query)
            total_saved += result.get('saved_items', 0) if isinstance(result.get('saved_items'), int) else len(result.get('saved_items', []))
            results.append(result)
            
            # í‚¤ì›Œë“œ ê°„ ë”œë ˆì´ (API ì œí•œ ê³ ë ¤)
            if i < len(queries) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´
                time.sleep(1)
        
        return CrawlResponse(
            statusCode=200,
            body={
                "message": f"ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ: {len(queries)}ê°œ í‚¤ì›Œë“œ, {total_saved}ê°œ ë‰´ìŠ¤ ì €ì¥",
                "total_keywords": len(queries),
                "total_saved": total_saved,
                "duration_seconds": round(time.time() - start_time, 2),
                "results": results,
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/crawl/status")
async def get_crawl_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    
    # ìµœì‹  í†µê³„ ì¡°íšŒ
    stats = db_manager.get_crawl_statistics()
    
    return {
        "statusCode": 200,
        "body": {
            "crawl_status": {
                "is_running": crawl_status.is_running,
                "last_run": crawl_status.last_run,
                "total_collected": stats['total_items'],  # DBì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ
                "last_query": crawl_status.last_query,
                "last_error": crawl_status.last_error
            },
            "database_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    }

# Auto Scaling í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/cpu-intensive")
async def cpu_intensive_crawl():
    """CPU ì§‘ì•½ì  ì‘ì—… (ì‹¤ì œ í¬ë¡¤ë§ + CPU ë¶€í•˜)"""
    
    start_time = time.time()
    
    try:
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
        result = crawl_news_sync("AI", 5)
        
        # ì¶”ê°€ CPU ë¶€í•˜ ìƒì„± (3ì´ˆê°„)
        cpu_start = time.time()
        while time.time() - cpu_start < 3:
            _ = sum(range(100000))
        
        return {
            "statusCode": 200,
            "body": {
                "message": "CPU intensive crawling completed",
                "duration_seconds": round(time.time() - start_time, 2),
                "crawl_result": result,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        return {
            "statusCode": 500,
            "body": {
                "message": "CPU intensive task failed",
                "error": str(e),
                "duration_seconds": round(time.time() - start_time, 2),
                "timestamp": datetime.now().isoformat()
            }
        }

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8001)),
        reload=True
    )