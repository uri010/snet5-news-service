from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import os
import time
from datetime import datetime
from typing import Optional, List
import asyncio
import concurrent.futures

from models import CrawlRequest, CrawlResponse, CrawlStatus
from naver_api import naver_api
from database import db_manager
from image_extractor import image_extractor

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="News Data Collection Service",
    description="ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ì´ë¯¸ì§€ í¬í•¨)",
    version="1.1.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
crawl_status = CrawlStatus(
    is_running=False,
    last_run=None,
    total_collected=0,
    last_query=None,
    last_error=None
)

@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ì‹œ DynamoDB ì—°ê²°"""
    try:
        db_manager.connect()
        stats = db_manager.get_crawl_statistics()
        crawl_status.total_collected = stats['total_items']
        print(f"ğŸ“ˆ ê¸°ì¡´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {stats['total_items']}ê°œ")
        
        # ì´ë¯¸ì§€ ì¶”ì¶œê¸° ìƒíƒœ í™•ì¸
        if image_extractor.s3_client:
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜ì§‘ ê¸°ëŠ¥ í™œì„±í™” (S3: {image_extractor.s3_bucket})")
        else:
            print(f"âš ï¸  ì´ë¯¸ì§€ ìˆ˜ì§‘ ê¸°ëŠ¥ ë¹„í™œì„±í™” (S3 ì„¤ì • ì—†ìŒ)")
            
    except Exception as e:
        print(f"âŒ ì‹œì‘ ì‹œ ì˜¤ë¥˜: {e}")

@app.get("/")
async def root():
    return {
        "service": "News Data Collection Service",
        "version": "1.1.0",
        "description": "ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (ì´ë¯¸ì§€ í¬í•¨)",
        "features": {
            "news_collection": True,
            "image_extraction": image_extractor.s3_client is not None,
            "s3_storage": image_extractor.s3_bucket if image_extractor.s3_client else None,
            "cloudfront_cdn": image_extractor.cloudfront_domain if image_extractor.s3_client else None
        },
        "endpoints": [
            "POST /api/crawl/start - ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ì´ë¯¸ì§€ í¬í•¨)",
            "POST /api/crawl/batch - ë°°ì¹˜ ìˆ˜ì§‘ (ì´ë¯¸ì§€ í¬í•¨)",
            "GET /api/crawl/status - ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ",
            "GET /health - í—¬ìŠ¤ì²´í¬"
        ]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "data-collection-service",
        "version": "1.1.0",
        "naver_api": "connected" if naver_api.client_id else "not_configured",
        "dynamodb": "connected" if db_manager.table else "not_connected",
        "image_service": {
            "enabled": image_extractor.s3_client is not None,
            "s3_bucket": image_extractor.s3_bucket if image_extractor.s3_client else None,
            "cloudfront_domain": image_extractor.cloudfront_domain if image_extractor.s3_client else None
        }
    }

def process_single_news_image(news_item: dict) -> dict:
    """ë‹¨ì¼ ë‰´ìŠ¤ ì•„ì´í…œì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬"""
    try:
        # ë„¤ì´ë²„ API ì‘ë‹µì—ì„œ ì˜¬ë°”ë¥¸ í•„ë“œ ì‚¬ìš©
        originallink = news_item.get('originallink')  # ì›ë³¸ ê¸°ì‚¬ URL (ì´ë¯¸ì§€ ì¶”ì¶œìš©)
        news_id = news_item.get('id')  # ë‰´ìŠ¤ ID
        title = news_item.get('title', 'ì œëª©ì—†ìŒ')
        
        if not originallink or not news_id:
            news_item['image_url'] = None
            news_item['cloudfront_image_url'] = None
            return news_item
            
        print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬: {title[:40]}...")
        
        # originallinkì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        image_result = image_extractor.process_news_image(originallink, str(news_id))
        
        if image_result:
            # NewsItem ëª¨ë¸ì— ë§ëŠ” í•„ë“œëª…ìœ¼ë¡œ ì €ì¥
            news_item['image_url'] = image_result['original_url']  # ì›ë³¸ ì´ë¯¸ì§€ URL
            news_item['cloudfront_image_url'] = image_result['cloudfront_url']  # CloudFront URL
            print(f"âœ… ì´ë¯¸ì§€ ì™„ë£Œ: CloudFront URL ìƒì„±ë¨")
        else:
            news_item['image_url'] = None
            news_item['cloudfront_image_url'] = None
            print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ")
            
        return news_item
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì˜¤ë¥˜: {str(e)[:50]}...")
        news_item['image_url'] = None
        news_item['cloudfront_image_url'] = None
        return news_item

async def process_news_images_concurrently(news_items: List[dict], max_workers: int = 3) -> List[dict]:
    """ë‰´ìŠ¤ ì•„ì´í…œë“¤ì˜ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    if not image_extractor.s3_client:
        print("âš ï¸  S3 ë¯¸ì„¤ì • - ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆëœ€")
        for item in news_items:
            item['image_url'] = None
            item['cloudfront_image_url'] = None
        return news_items
    
    print(f"ğŸ–¼ï¸  {len(news_items)}ê°œ ë‰´ìŠ¤ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    loop = asyncio.get_event_loop()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ë‰´ìŠ¤ ì•„ì´í…œì˜ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
        tasks = [
            loop.run_in_executor(executor, process_single_news_image, news_item)
            for news_item in news_items
        ]
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        processed_items = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ê°€ ë°œìƒí•œ ê²½ìš° ì›ë³¸ ì•„ì´í…œ ë°˜í™˜
        final_items = []
        for i, result in enumerate(processed_items):
            if isinstance(result, Exception):
                print(f"âŒ ë‰´ìŠ¤ #{i} ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
                original_item = news_items[i].copy()
                original_item['image_url'] = None
                original_item['cloudfront_image_url'] = None
                final_items.append(original_item)
            else:
                final_items.append(result)
    
    # ì„±ê³µí•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ê°œìˆ˜ ê³„ì‚°
    success_count = sum(1 for item in final_items if item.get('cloudfront_image_url') is not None)
    print(f"âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(news_items)}ê°œ ì„±ê³µ")
    
    return final_items

async def crawl_news_async(query: str, display: int = 10, start: int = 1, sort: str = "date", include_images: bool = True) -> dict:
    """ë¹„ë™ê¸° ë°©ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§ - ì´ë¯¸ì§€ í¬í•¨"""
    crawl_status.is_running = True
    crawl_status.last_query = query
    crawl_status.last_error = None
    
    try:
        start_time = time.time()
        print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: '{query}' (display={display}, images={'enabled' if include_images else 'disabled'})")
        
        # ë„¤ì´ë²„ API í˜¸ì¶œ
        news_data = naver_api.search_news(query=query, display=display, start=start, sort=sort)
        
        # DynamoDB í˜•íƒœë¡œ ë³€í™˜
        db_items = naver_api.format_for_dynamodb(news_data, query)
        
        # ë””ë²„ê¹…: ë³€í™˜ëœ ë°ì´í„° êµ¬ì¡° í™•ì¸ (ì²« ì‹¤í–‰ì‹œë§Œ)
        if db_items:
            print(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘: {len(db_items)}ê°œ ì•„ì´í…œ, API ì´ ê²°ê³¼: {news_data.get('total', 0)}ê°œ")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        if include_images and db_items:
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
            db_items = await process_news_images_concurrently(db_items)
        else:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•ŠëŠ” ê²½ìš° image í•„ë“œë“¤ì„ Noneìœ¼ë¡œ ì„¤ì •
            for item in db_items:
                item['image_url'] = None
                item['cloudfront_image_url'] = None
        
        # DynamoDBì— ì €ì¥
        save_result = db_manager.save_news_items(db_items)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        crawl_status.total_collected += save_result['saved_count']
        crawl_status.last_run = datetime.now().isoformat()
        
        duration = time.time() - start_time
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        image_stats = {
            'enabled': include_images,
            'success_count': 0,
            'total_processed': 0
        }
        
        if include_images:
            image_stats['total_processed'] = len(db_items)
            image_stats['success_count'] = sum(1 for item in db_items if item.get('cloudfront_image_url') is not None)
        
        result = {
            'message': f'Successfully saved {save_result["saved_count"]} news items to DynamoDB',
            'search_query': query,
            'collected_at': datetime.now().isoformat(),
            'total_results': news_data.get('total', 0),
            'saved_items': save_result['saved_items'],
            'failed_count': save_result['failed_count'],
            'duration_seconds': round(duration, 2),
            'image_processing': image_stats
        }
        
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {save_result['saved_count']}ê°œ ì €ì¥, ì´ë¯¸ì§€ {image_stats['success_count']}/{image_stats['total_processed']}ê°œ ì²˜ë¦¬, {duration:.2f}ì´ˆ ì†Œìš”")
        return result
        
    except Exception as e:
        error_msg = f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        crawl_status.last_error = error_msg
        print(f"âŒ {error_msg}")
        raise Exception(error_msg)
    finally:
        crawl_status.is_running = False

def crawl_news_sync(query: str, display: int = 10, start: int = 1, sort: str = "date", include_images: bool = True) -> dict:
    """ë™ê¸° ë°©ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš©) - ì´ë¯¸ì§€ í¬í•¨"""
    crawl_status.is_running = True
    crawl_status.last_query = query
    crawl_status.last_error = None
    
    try:
        start_time = time.time()
        print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: '{query}' (display={display}, images={'enabled' if include_images else 'disabled'})")
        
        # ë„¤ì´ë²„ API í˜¸ì¶œ
        news_data = naver_api.search_news(query=query, display=display, start=start, sort=sort)
        
        # DynamoDB í˜•íƒœë¡œ ë³€í™˜
        db_items = naver_api.format_for_dynamodb(news_data, query)
        
        # ë””ë²„ê¹…: ë³€í™˜ëœ ë°ì´í„° êµ¬ì¡° í™•ì¸ (ì²« ì‹¤í–‰ì‹œë§Œ)
        if db_items:
            print(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘: {len(db_items)}ê°œ ì•„ì´í…œ, API ì´ ê²°ê³¼: {news_data.get('total', 0)}ê°œ")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
        if include_images and db_items:
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
            processed_items = []
            success_count = 0
            
            for i, item in enumerate(db_items):
                processed_item = process_single_news_image(item)
                processed_items.append(processed_item)
                if processed_item.get('cloudfront_image_url') is not None:
                    success_count += 1
            
            db_items = processed_items
            print(f"âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(db_items)}ê°œ ì„±ê³µ")
        else:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•ŠëŠ” ê²½ìš° image í•„ë“œë“¤ì„ Noneìœ¼ë¡œ ì„¤ì •
            for item in db_items:
                item['image_url'] = None
                item['cloudfront_image_url'] = None
        
        # DynamoDBì— ì €ì¥
        save_result = db_manager.save_news_items(db_items)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        crawl_status.total_collected += save_result['saved_count']
        crawl_status.last_run = datetime.now().isoformat()
        
        duration = time.time() - start_time
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        image_stats = {
            'enabled': include_images,
            'success_count': 0,
            'total_processed': 0
        }
        
        if include_images:
            image_stats['total_processed'] = len(db_items)
            image_stats['success_count'] = sum(1 for item in db_items if item.get('cloudfront_image_url') is not None)
        
        result = {
            'message': f'Successfully saved {save_result["saved_count"]} news items to DynamoDB',
            'search_query': query,
            'collected_at': datetime.now().isoformat(),
            'total_results': news_data.get('total', 0),
            'saved_items': save_result['saved_items'],
            'failed_count': save_result['failed_count'],
            'duration_seconds': round(duration, 2),
            'image_processing': image_stats
        }
        
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {save_result['saved_count']}ê°œ ì €ì¥, ì´ë¯¸ì§€ {image_stats['success_count']}/{image_stats['total_processed']}ê°œ ì²˜ë¦¬, {duration:.2f}ì´ˆ ì†Œìš”")
        return result
        
    except Exception as e:
        error_msg = f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        crawl_status.last_error = error_msg
        print(f"âŒ {error_msg}")
        raise Exception(error_msg)
    finally:
        crawl_status.is_running = False

@app.post("/api/crawl/start", response_model=CrawlResponse)
async def start_crawling(
    background_tasks: BackgroundTasks,
    query: str = Query("ë¹„íŠ¸ì½”ì¸", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    display: int = Query(10, ge=1, le=100, description="ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    start: int = Query(1, ge=1, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜"),
    sort: str = Query("date", description="ì •ë ¬ ë°©ì‹ (sim: ì •í™•ë„ìˆœ, date: ë‚ ì§œìˆœ)"),
    include_images: bool = Query(True, description="ì´ë¯¸ì§€ ìˆ˜ì§‘ ì—¬ë¶€")
):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰) - ì´ë¯¸ì§€ í¬í•¨"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    def background_crawl():
        try:
            crawl_news_sync(query, display, start, sort, include_images)
        except Exception as e:
            print(f"ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    background_tasks.add_task(background_crawl)
    
    return CrawlResponse(
        statusCode=200,
        body={
            "message": f"'{query}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "query": query,
            "display": display,
            "sort": sort,
            "include_images": include_images,
            "image_service_enabled": image_extractor.s3_client is not None,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.post("/api/crawl/now", response_model=CrawlResponse)
async def crawl_now(
    query: str = Query("ë¹„íŠ¸ì½”ì¸", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    display: int = Query(10, ge=1, le=100, description="ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    start: int = Query(1, ge=1, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜"),
    sort: str = Query("date", description="ì •ë ¬ ë°©ì‹"),
    include_images: bool = Query(True, description="ì´ë¯¸ì§€ ìˆ˜ì§‘ ì—¬ë¶€")
):
    """ì¦‰ì‹œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë¹„ë™ê¸° ì‹¤í–‰) - ì´ë¯¸ì§€ í¬í•¨"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    try:
        result = await crawl_news_async(query, display, start, sort, include_images)
        
        return CrawlResponse(
            statusCode=200,
            body=result
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/crawl/batch")
async def batch_crawling(
    queries: List[str] = Query(["ë¹„íŠ¸ì½”ì¸"], description="ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡"),
    display_per_query: int = Query(5, ge=1, le=50, description="í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    include_images: bool = Query(True, description="ì´ë¯¸ì§€ ìˆ˜ì§‘ ì—¬ë¶€")
):
    """ë°°ì¹˜ ìˆ˜ì§‘ (ì—¬ëŸ¬ í‚¤ì›Œë“œ, CPU ì§‘ì•½ì ) - ì´ë¯¸ì§€ í¬í•¨"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    start_time = time.time()
    total_saved = 0
    total_images_processed = 0
    total_images_success = 0
    results = []
    
    try:
        for i, query in enumerate(queries):
            print(f"ğŸ” ë°°ì¹˜ ìˆ˜ì§‘ [{i+1}/{len(queries)}]: '{query}'")
            
            result = await crawl_news_async(query, display_per_query, include_images=include_images)
            total_saved += result.get('saved_items', 0) if isinstance(result.get('saved_items'), int) else len(result.get('saved_items', []))
            
            # ì´ë¯¸ì§€ í†µê³„ ëˆ„ì 
            if 'image_processing' in result:
                total_images_processed += result['image_processing'].get('total_processed', 0)
                total_images_success += result['image_processing'].get('success_count', 0)
            
            results.append(result)
            
            # í‚¤ì›Œë“œ ê°„ ë”œë ˆì´ (API ì œí•œ ê³ ë ¤)
            if i < len(queries) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´
                await asyncio.sleep(1)
        
        return CrawlResponse(
            statusCode=200,
            body={
                "message": f"ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ: {len(queries)}ê°œ í‚¤ì›Œë“œ, {total_saved}ê°œ ë‰´ìŠ¤ ì €ì¥",
                "total_keywords": len(queries),
                "total_saved": total_saved,
                "duration_seconds": round(time.time() - start_time, 2),
                "image_processing": {
                    "enabled": include_images,
                    "total_processed": total_images_processed,
                    "success_count": total_images_success,
                    "success_rate": round(total_images_success / total_images_processed * 100, 1) if total_images_processed > 0 else 0
                },
                "results": results,
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/crawl/status")
async def get_crawl_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    
    # ìµœì‹  í†µê³„ ì¡°íšŒ
    stats = db_manager.get_crawl_statistics()
    
    return {
        "statusCode": 200,
        "body": {
            "crawl_status": {
                "is_running": crawl_status.is_running,
                "last_run": crawl_status.last_run,
                "total_collected": stats['total_items'],  # DBì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ
                "last_query": crawl_status.last_query,
                "last_error": crawl_status.last_error
            },
            "database_stats": stats,
            "image_service": {
                "enabled": image_extractor.s3_client is not None,
                "s3_bucket": image_extractor.s3_bucket if image_extractor.s3_client else None,
                "cloudfront_domain": image_extractor.cloudfront_domain if image_extractor.s3_client else None
            },
            "timestamp": datetime.now().isoformat()
        }
    }

# Auto Scaling í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/cpu-intensive")
async def cpu_intensive_crawl():
    """CPU ì§‘ì•½ì  ì‘ì—… (ì‹¤ì œ í¬ë¡¤ë§ + ì´ë¯¸ì§€ ì²˜ë¦¬ + CPU ë¶€í•˜)"""
    
    start_time = time.time()
    
    try:
        # ì‹¤ì œ í¬ë¡¤ë§ + ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤í–‰
        result = await crawl_news_async("AI", 5, include_images=True)
        
        # ì¶”ê°€ CPU ë¶€í•˜ ìƒì„± (3ì´ˆê°„)
        cpu_start = time.time()
        while time.time() - cpu_start < 3:
            _ = sum(range(100000))
        
        return {
            "statusCode": 200,
            "body": {
                "message": "CPU intensive crawling with image processing completed",
                "duration_seconds": round(time.time() - start_time, 2),
                "crawl_result": result,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        return {
            "statusCode": 500,
            "body": {
                "message": "CPU intensive task failed",
                "error": str(e),
                "duration_seconds": round(time.time() - start_time, 2),
                "timestamp": datetime.now().isoformat()
            }
        }

# ì´ë¯¸ì§€ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/image/test")
async def test_image_extraction(
    url: str = Query(..., description="í…ŒìŠ¤íŠ¸í•  ë‰´ìŠ¤ ê¸°ì‚¬ URL")
):
    """ì´ë¯¸ì§€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    try:
        if not image_extractor.s3_client:
            raise HTTPException(status_code=503, detail="S3ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„ì‹œ UID ìƒì„±
        import hashlib
        temp_uid = hashlib.md5(url.encode()).hexdigest()[:10]
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        result = image_extractor.process_news_image(url, f"test_{temp_uid}")
        
        return {
            "statusCode": 200,
            "body": {
                "message": "ì´ë¯¸ì§€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
                "test_url": url,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8001)),
        reload=True
    )