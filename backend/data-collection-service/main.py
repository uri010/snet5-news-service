from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import os
import time
from datetime import datetime
from typing import Optional
import asyncio
import email.utils
import concurrent.futures

from models import CrawlResponse, CrawlStatus
from naver_api import naver_api
from database import db_manager
from image_extractor import image_extractor

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="News Data Collection Service",
    description="ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ê°„ì†Œí™” ë²„ì „)",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
crawl_status = CrawlStatus(
    is_running=False,
    last_run=None,
    total_collected=0,
    last_query=None,
    last_error=None
)

def is_news_newer(news_pub_date: str, latest_pub_date: str) -> bool:
    """ë‰´ìŠ¤ ë°œí–‰ì¼ì´ ê¸°ì¡´ ìµœì‹  ë‰´ìŠ¤ë³´ë‹¤ ë” ëŠ¦ì€ì§€ í™•ì¸"""
    try:
        # RFC-2822 í˜•ì‹ íŒŒì‹± (ì˜ˆ: "Mon, 09 Sep 2024 14:30:00 +0900")
        news_timestamp = email.utils.parsedate_to_datetime(news_pub_date)
        latest_timestamp = email.utils.parsedate_to_datetime(latest_pub_date)
        
        return news_timestamp > latest_timestamp
        
    except Exception as e:
        print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}, ë¬¸ìì—´ ë¹„êµë¡œ ëŒ€ì²´")
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ë¹„êµë¡œ ëŒ€ì²´
        return news_pub_date > latest_pub_date

@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ì‹œ DynamoDB ì—°ê²°"""
    try:
        db_manager.connect()
        stats = db_manager.get_crawl_statistics()
        crawl_status.total_collected = stats['total_items']
        print(f"ğŸ“ˆ ê¸°ì¡´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {stats['total_items']}ê°œ")
        
        if image_extractor.s3_client:
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜ì§‘ ê¸°ëŠ¥ í™œì„±í™”")
        else:
            print(f"âš ï¸  ì´ë¯¸ì§€ ìˆ˜ì§‘ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            
    except Exception as e:
        print(f"âŒ ì‹œì‘ ì‹œ ì˜¤ë¥˜: {e}")

@app.get("/")
async def root():
    return {
        "service": "News Data Collection Service",
        "version": "2.0.0",
        "description": "ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ì‹œê°„ ê¸°ë°˜ í•„í„°ë§)",
        "collection_logic": "DB ìµœì‹  ë‰´ìŠ¤ë³´ë‹¤ ë” ìµœì‹  ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘",
        "endpoints": [
            "POST /api/collect - ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰",
            "GET /api/status - ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ",
            "GET /health - í—¬ìŠ¤ì²´í¬"
        ]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "data-collection-service",
        "version": "2.0.0",
        "naver_api": "connected" if naver_api.client_id else "not_configured",
        "dynamodb": "connected" if db_manager.table else "not_connected",
        "image_service": image_extractor.s3_client is not None
    }

async def process_news_images_concurrently(news_items, max_workers: int = 3):
    """ë‰´ìŠ¤ ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬"""
    if not image_extractor.s3_client:
        for item in news_items:
            item['image_url'] = None
            item['cloudfront_image_url'] = None
        return news_items
    
    def process_single_image(news_item):
        try:
            originallink = news_item.get('originallink')
            news_id = news_item.get('id')
            
            if originallink and news_id:
                result = image_extractor.process_news_image(originallink, str(news_id))
                if result:
                    news_item['image_url'] = result['original_url']
                    news_item['cloudfront_image_url'] = result['cloudfront_url']
                    return news_item
            
            news_item['image_url'] = None
            news_item['cloudfront_image_url'] = None
            return news_item
            
        except Exception:
            news_item['image_url'] = None
            news_item['cloudfront_image_url'] = None
            return news_item
    
    loop = asyncio.get_event_loop()
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        tasks = [
            loop.run_in_executor(executor, process_single_image, item)
            for item in news_items
        ]
        return await asyncio.gather(*tasks, return_exceptions=False)

async def collect_news_with_time_filter(query: str, display: int = 10, start: int = 1, sort: str = "date", include_images: bool = True) -> dict:
    """ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ì„ ì ìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    crawl_status.is_running = True
    crawl_status.last_query = query
    crawl_status.last_error = None
    
    try:
        start_time = time.time()
        print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: '{query}' (display={display}, images={'enabled' if include_images else 'disabled'})")
        
        # DBì—ì„œ ê°€ì¥ ìµœì‹  pubDate ì¡°íšŒ (í•˜ë‚˜ë§Œ)
        latest_pub_date = db_manager.get_last_collected_time()
        if latest_pub_date:
            print(f"ğŸ“… DB ìµœì‹  ë‰´ìŠ¤ ì‹œê°„: {latest_pub_date}")
        else:
            print(f"ğŸ“… ì²« ë²ˆì§¸ ìˆ˜ì§‘ - ì „ì²´ ìˆ˜ì§‘ì„ ì§„í–‰í•©ë‹ˆë‹¤")
        
        # ë„¤ì´ë²„ API í˜¸ì¶œ
        news_data = naver_api.search_news(
            query=query, 
            display=display,
            start=start, 
            sort=sort
        )
        
        # DynamoDB í˜•íƒœë¡œ ë³€í™˜
        db_items = naver_api.format_for_dynamodb(news_data, query)
        
        # ìµœì‹  ë‰´ìŠ¤ ì‹œê°„ê³¼ ë¹„êµí•˜ì—¬ ë” ìµœì‹  ë‰´ìŠ¤ë§Œ í•„í„°ë§
        original_count = len(db_items)
        if latest_pub_date and db_items:
            print(f"ğŸ” ìµœì‹  ë‰´ìŠ¤ì™€ ë‚ ì§œ ë¹„êµ ì‹œì‘: ê¸°ì¤€ {latest_pub_date}")
            filtered_items = []
            
            for item in db_items:
                news_pub_date = item.get('pubDate', '')
                if not news_pub_date:
                    # pubDateê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì¼ë‹¨ ìˆ˜ì§‘
                    filtered_items.append(item)
                    print(f"  âœ… ìˆ˜ì§‘: {item.get('title', 'Unknown')[:50]}... (pubDate ì—†ìŒ)")
                    continue
                
                # ê°€ì¥ ìµœì‹  ë‰´ìŠ¤ì™€ë§Œ ë¹„êµ
                if is_news_newer(news_pub_date, latest_pub_date):
                    filtered_items.append(item)
                    print(f"  âœ… ìˆ˜ì§‘: {item.get('title', 'Unknown')[:50]}... ({news_pub_date})")
                else:
                    print(f"  â­ï¸  ìŠ¤í‚µ: {item.get('title', 'Unknown')[:50]}... (ê¸°ì¡´ë³´ë‹¤ ì˜¤ë˜ë¨)")
            
            db_items = filtered_items
            print(f"ğŸ• ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {original_count}ê°œ â†’ {len(db_items)}ê°œ (ë” ìµœì‹  ë‰´ìŠ¤ë§Œ)")
        else:
            print(f"ğŸ“Š ì²« ìˆ˜ì§‘ ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: {len(db_items)}ê°œ ëª¨ë‘ ì²˜ë¦¬")
        
        if not db_items:
            message = "ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤" if latest_pub_date else "ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤"
            print(f"âš ï¸ {message}")
            return {
                'message': message,
                'search_query': query,
                'saved_count': 0,
                'latest_db_news_time': latest_pub_date,
                'original_fetched': original_count,
                'filtered_count': len(db_items),
                'duration_seconds': round(time.time() - start_time, 2)
            }
        
        print(f"ğŸ“Š ì²˜ë¦¬í•  ë‰´ìŠ¤: {len(db_items)}ê°œ")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ (ë³‘ë ¬)
        if include_images and image_extractor.s3_client:
            db_items = await process_news_images_concurrently(db_items)
        else:
            for item in db_items:
                item['image_url'] = None
                item['cloudfront_image_url'] = None
        
        # DynamoDBì— ì €ì¥
        save_result = db_manager.save_news_items(db_items)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        crawl_status.total_collected += save_result['saved_count']
        crawl_status.last_run = datetime.now().isoformat()
        
        duration = time.time() - start_time
        
        # ì´ë¯¸ì§€ í†µê³„
        image_success = sum(1 for item in db_items if item.get('cloudfront_image_url'))
        
        result = {
            'message': f'Successfully collected {save_result["saved_count"]} new news for "{query}"',
            'search_query': query,
            'collected_at': datetime.now().isoformat(),
            'latest_db_news_time': latest_pub_date,
            'original_fetched': original_count,
            'filtered_count': len(db_items),
            'saved_count': save_result['saved_count'],
            'failed_count': save_result.get('failed_count', 0),
            'images_processed': image_success,
            'duration_seconds': round(duration, 2)
        }
        
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {save_result['saved_count']}ê°œ ì €ì¥, {duration:.2f}ì´ˆ")
        return result
        
    except Exception as e:
        error_msg = f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        crawl_status.last_error = error_msg
        print(f"âŒ {error_msg}")
        raise Exception(error_msg)
    finally:
        crawl_status.is_running = False

@app.post("/api/collect", response_model=CrawlResponse)
async def collect_news(
    query: str = Query("ë¹„íŠ¸ì½”ì¸", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    display: int = Query(10, ge=1, le=100, description="ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜"),
    start: int = Query(1, ge=1, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜"),
    sort: str = Query("date", description="ì •ë ¬ ë°©ì‹ (sim: ì •í™•ë„ìˆœ, date: ë‚ ì§œìˆœ)"),
    include_images: bool = Query(True, description="ì´ë¯¸ì§€ ìˆ˜ì§‘ ì—¬ë¶€")
):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰ (ê¸°ì¡´ API í˜¸í™˜)"""
    
    if crawl_status.is_running:
        raise HTTPException(status_code=400, detail="ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
    
    try:
        result = await collect_news_with_time_filter(query, display, start, sort, include_images)
        
        return CrawlResponse(
            statusCode=200,
            body=result
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status")
async def get_status():
    """ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ"""
    stats = db_manager.get_crawl_statistics()
    
    return {
        "statusCode": 200,
        "body": {
            "service_status": {
                "is_running": crawl_status.is_running,
                "last_run": crawl_status.last_run,
                "last_error": crawl_status.last_error
            },
            "collection_stats": {
                "total_collected": stats['total_items']
            },
            "services": {
                "naver_api": "connected" if naver_api.client_id else "not_configured",
                "dynamodb": "connected" if db_manager.table else "not_connected",
                "image_processing": image_extractor.s3_client is not None
            },
            "timestamp": datetime.now().isoformat()
        }
    }

@app.get("/api/stress-test")
async def stress_test():
    """CPU ë¶€í•˜ + ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    start_time = time.time()
    
    try:
        # ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ê¸°ë³¸ê°’ ì‚¬ìš©)
        result = await collect_news_with_time_filter("AI", 5)
        
        # CPU ë¶€í•˜ ì¶”ê°€
        cpu_start = time.time()
        while time.time() - cpu_start < 5:
            _ = sum(range(200000))
        
        return {
            "statusCode": 200,
            "body": {
                "message": "Stress test completed with news collection",
                "duration_seconds": round(time.time() - start_time, 2),
                "collection_result": result,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        return {
            "statusCode": 500,
            "body": {
                "error": str(e),
                "duration_seconds": round(time.time() - start_time, 2)
            }
        }

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8001)),
        reload=True
    )